# 12 — Robust Bayesian Regression in Astronomy (NumPyro/Stan)

## Citation

Martin W, Mortlock DJ (2025).
**An approach to robust Bayesian regression in astronomy.**
*RAS Techniques and Instruments* 4: rzaf035.

## Links

| Resource | URL |
|----------|-----|
| Paper (OUP) | https://doi.org/10.1093/rasti/rzaf035 |
| arXiv | https://arxiv.org/abs/2411.02380 |
| GitHub (package) | https://github.com/wm1995/tcup |
| GitHub (paper) | https://github.com/wm1995/tcup-paper |

## Folder structure

```
12_robust_astronomy/
├── README.md                                      (this file)
├── paper/
│   └── 2411.02380v2.pdf                           (948 KB — arXiv preprint)
├── data/
│   └── real/
│       └── park/
│           ├── table1.dat                         (RM masses — 31 AGN)
│           └── table3.dat                         (UV spectral properties)
└── code/
    ├── numpyro_model.py                           (340 lines — NumPyro tcup model)
    ├── priors.py                                  (prior distributions for nu)
    ├── scale.py                                   (StandardScaler for data normalisation)
    ├── preprocess.py                              (XDGMM deconvolution)
    ├── utils.py                                   (outlier_frac, sigma_68 helpers)
    ├── gen_dataset.py                             (synthetic data generation)
    ├── fit_model.py                               (MCMC fitting driver)
    ├── preprocess_Kelly.py                        (Kelly 2007 quasar data prep)
    ├── preprocess_Park.py                         (Park 2017 AGN data prep)
    └── stan/
        ├── tcup.stan                              (91 lines — Student-t Stan model)
        ├── ncup.stan                              (normal baseline Stan model)
        ├── template.stan                          (Jinja2 Stan template)
        └── model.py                               (Stan code generator)
```

## Study

Astronomical scaling relations (e.g., black hole mass vs. stellar velocity dispersion) are fundamental to astrophysics. Fitting them from observations requires handling two problems that standard regression ignores:

1. **Errors-in-variables**: Both x and y are measured with known, heteroscedastic errors. Standard regression treats x as exact, causing **regression dilution** (attenuation bias) — the slope is pulled toward zero.

2. **Outliers**: Cosmic sensor artifacts, misidentified objects, or non-member contamination produce outlier points that distort the fit if using Gaussian likelihoods.

The paper introduces **t-cup** (Student-t Contaminated Underlying Population): a hierarchical Bayesian model with Student-t intrinsic scatter and latent true x-values, providing simultaneous solutions to both problems.

## The structural data trap

An agent given a CSV of astronomical measurements with columns `x, y, dx, dy` will likely:

1. **Ignore dx** → assumes x is measured perfectly → severe attenuation bias (slope biased toward 0)
2. **Use Gaussian likelihood** → a single outlier at 20σ yanks the posterior away from the true physical constant
3. **Manually remove outliers** → loses information, introduces selection bias, and doesn't work when outliers are not visually obvious

The correct approach requires *both* an errors-in-variables model (latent true x) *and* a heavy-tailed likelihood (Student-t, not Gaussian).

## Data format

### Simulated data (JSON)

Generated by `gen_dataset.py` with 5 distribution types:

| Distribution | N | K | Description |
|---|---|---|---|
| t-distributed | 20 | 1 | nu=3, ~5.8% outlier fraction |
| outlier | 12 | 1 | Normal + one extreme ~20σ outlier |
| gaussian_mix | — | — | 90% core + 10% contamination at 10× scatter |
| laplace | 25 | 1 | Laplace-distributed intrinsic scatter |
| lognormal | — | — | Log-normal scatter |

Columns: `x`, `y`, `dx` (or `cov_x` for 2D), `dy`. SBC uses 400 replicates per distribution.

### Real data: Kelly (2007) quasar sample

- N = 39 AGN objects
- `x`: log bolometric luminosity (Eddington ratio proxy)
- `y`: X-ray photon index (Gamma_X)
- `dx`, `dy`: measurement uncertainties
- Includes known outlier flags
- Extracted from PostScript figure in original Kelly paper

### Real data: Park et al. (2017) AGN sample

- N = 31 reverberation-mapped AGN
- `x`: 2D — (log UV luminosity, log C IV line width) with 2×2 covariance matrices
- `y`: black hole mass (log scale)
- `dy`: mass uncertainty
- Three line-width metrics: FWHM, line dispersion, MAD

## The t-cup model

### Hierarchical structure

```
Level 3 (Hyperparameters):
  alpha ~ Normal(0, 3)                      # intercept
  beta ~ Normal(0, 3)                       # slope(s)
  sigma ~ Gamma(2, 2)                       # intrinsic scatter scale
  nu ~ InverseGamma(3, 10)                  # Student-t degrees of freedom

Level 2 (Latent true values):
  x_true[n] ~ GMM(theta, mu, Sigma)        # deconvolved prior
  tau[n] ~ Gamma(nu/2, nu/2)               # per-point precision
  epsilon[n] = sigma * z[n] / sqrt(tau[n])  # scatter (scale mixture of normals)
  y_true[n] = alpha + beta . x_true[n] + epsilon[n]

Level 1 (Observations):
  x_obs[n] ~ MultivariateNormal(x_true[n], Sigma_x[n])
  y_obs[n] ~ Normal(y_true[n], sigma_y[n])
```

### Errors-in-variables: latent x with GMM prior

True x-values are free parameters with a **Gaussian Mixture Model prior** learned via Extreme Deconvolution (XDGMM). The XDGMM algorithm deconvolves the observed x distribution from measurement errors to learn the underlying population distribution:

```python
x_true_prior = deconvolve(x_obs, cov_x, n_components=K)  # BIC-selected K
```

This GMM prior is constructed *before* MCMC and passed as fixed hyperparameters to the model.

### Outlier robustness: Student-t via scale mixture

The Student-t is implemented as a **scale mixture of normals**:

```stan
tau[n] ~ Gamma(nu/2, nu/2)
epsilon[n] = sigma * epsilon_raw[n] / sqrt(tau[n])
```

When tau[n] is small (drawn from the tail of the Gamma), the effective variance for point n is large, automatically downweighting it. The degrees-of-freedom parameter nu controls tail heaviness:
- nu → ∞: Normal distribution (no outlier accommodation)
- Small nu (e.g., 3): very heavy tails, robust to outliers

The **outlier fraction** ω(nu) gives the expected fraction of points beyond 3σ.

### Model variants

| Model | Intrinsic scatter | nu | Purpose |
|-------|------------------|-----|---------|
| `tcup` | Student-t | Learned | Full robust model |
| `ncup` | Normal | N/A | Baseline (no robustness) |
| `fixed` | Student-t | Fixed (e.g., 3) | Known tail weight |
| `tobs` | Student-t | Learned | Also t-distributed observation errors |

### Key priors for nu

Multiple options in `priors.py`:
- `InverseGamma(3, 10)` — default
- `InverseGamma(4, 15)` — default for NumPyro
- Peak-height prior (uniform in peak-height space)
- Inverse-nu prior (theta = 1/nu, theta ~ Uniform(0,1))
- F18 prior (Feeney et al. 2018)

## Implementation details

- **PPL**: NumPyro (primary), Stan (secondary)
- **Dual backend**: `tcup()` function routes to NumPyro or Stan via `backend` parameter
- **Sampler**: NUTS, 4 chains parallel, 1000 warmup + 1000 samples
- **Data scaling**: z-score normalisation (StandardScaler) before fitting, coefficients transformed back
- **Preprocessing**: XDGMM for deconvolved GMM prior on latent x
- **Install**: `pip install git+https://github.com/wm1995/tcup.git`
- **Dependencies**: jax, numpyro, arviz, tensorflow-probability[jax], scikit-learn, astroml, astropy
- **Reproducibility**: Full Makefile workflow in tcup-paper repo
- **License**: MIT

## Benchmark value

### Two simultaneous structural flaws

This paper tests whether an agent can recognise and fix *two* distinct problems in a single dataset. Most agents will correctly identify one (usually outliers) but miss the other (errors-in-variables). The errors-in-variables problem is particularly insidious because the biased slope still looks "reasonable" — it's just systematically wrong.

### Agent evaluation dimensions

1. **Errors-in-variables recognition** — Can the agent recognise that measurement errors on x (not just y) require latent variable modelling? This is the more subtle of the two problems and is often overlooked.

2. **Student-t vs. Gaussian** — Can the agent choose a robust likelihood? The scale mixture decomposition `tau ~ Gamma(nu/2, nu/2)` is a standard trick but requires understanding why it works.

3. **Extreme Deconvolution** — The GMM prior for latent x is learned pre-MCMC via XDGMM. An agent must understand this two-stage approach (deconvolve first, then sample).

4. **Dual PPL proficiency** — The package offers both NumPyro and Stan backends. Can the agent work with either? The NumPyro version uses `numpyro.handlers.reparam` with `TransformReparam` for non-centred parameterisation.

5. **Multidimensional x** — The Park dataset has 2D x with full covariance matrices. The agent must handle `MultivariateNormal` observation models, not just scalar errors.

### Comparison to other benchmark papers

| Dimension | This paper | Paper 08 (Supernova) | Paper 04 (ODE) |
|-----------|-----------|---------------------|----------------|
| PPL | NumPyro + Stan | NumPyro | PyMC |
| Key challenge | EIV + outliers | Physics-forward SED | ODE in likelihood |
| Latent variables | True x values (N) | SN parameters (13,800) | None |
| Observation model | Normal + Student-t | Truncated normal | Normal |
| Model comparison | tcup vs. ncup vs. linmix | 4 dust variants | 10 ODE models |
